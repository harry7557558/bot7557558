{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate all viable suffices through a dictionary search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dictionary and generate all prefices and suffices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "370105\n",
      "['a', 'aa', 'aaa', 'aah', 'aahed', 'aahing', 'aahs', 'aal', 'aalii', 'aaliis']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "req = requests.get(\"https://github.com/dwyl/english-words/raw/master/words_alpha.txt\")\n",
    "assert req.status_code == 200\n",
    "\n",
    "words = req.content.decode('utf-8').strip().replace('\\r', '').split('\\n')\n",
    "\n",
    "print(len(words))\n",
    "print(words[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1027816 1090688\n"
     ]
    }
   ],
   "source": [
    "loaded_prefices = set()\n",
    "loaded_suffices = set()\n",
    "for word in words:\n",
    "    for i in range(len(word)+1):\n",
    "        loaded_prefices.add(word[:i])\n",
    "        loaded_suffices.add(word[i:])\n",
    "loaded_prefices.remove('')\n",
    "loaded_suffices.remove('')\n",
    "print(len(loaded_prefices), len(loaded_suffices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a list of combined suffices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dedup(s: str):\n",
    "    \"\"\"Remove the consecutive letters of a word\"\"\"\n",
    "    # remove consecutive letters\n",
    "    t = \"\"\n",
    "    for c in s.lower():\n",
    "        if len(t) != 0 and c == t[-1]:\n",
    "            continue\n",
    "        t += c\n",
    "    return t\n",
    "\n",
    "def save_file(filename, fices):\n",
    "    fices = [dedup(word) for word in fices]\n",
    "    fices = sorted(list(set(fices)))\n",
    "    open(filename, 'w').write(' '.join(fices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14113 suffices generated\n",
      "['abilitable', 'abilitate', 'abilitated', 'abilitates', 'abilitating', 'abilitation', 'abilitationist', 'abilitations', 'abilitative', 'abilitator', 'abilities', 'ability', 'ablate', 'ablated', 'ablates', 'ablating', 'ablation', 'ablations', 'ablatival', 'ablative', 'ablatives', 'ablator', 'ablatores', 'able', 'abled', 'abledom', 'ableful', 'ablefuls', 'ableist', 'ableity', 'ableize', 'ableized', 'ableizing', 'ablement', 'ablements', 'abler', 'ableries', 'ablers', 'ablery', 'ables', 'ablesful', 'ableship', 'abless', 'ableted', 'abletic', 'ableting', 'ablets', 'ablety', 'ablier', 'ablies', 'ablin', 'abling', 'ablings', 'ablins', 'ablis', 'ablish', 'ablishable', 'ablished', 'ablisher', 'ablishes', 'ablishing', 'ablishment', 'ablishmentism', 'ablishments', 'ablize', 'ablized', 'ablizing', 'ably', 'acability', 'acable', 'acably', 'acal', 'acales', 'acals', 'acance', 'acancies', 'acancy', 'acatable', 'acate', 'acated', 'acater', 'acaters', 'acatery', 'acates', 'acating', 'acation', 'acational', 'acationed', 'acationer', 'acationers', 'acationing', 'acationist', 'acationists', 'acations', 'acative', 'aced', 'acedy', 'acen', 'acence', 'acencies']\n",
      "['entival', 'tesies', 'yer', 'storiate', 'yfied', 'anctionable', 'ictives', 'ineral', 'idoric', 'ortionates', 'acificated', 'intessence', 'sionment', 'ancering', 'istish', 'alencing', 'ortiate', 'ionicities', 'ternes', 'tored', 'sitatory', 'ortability', 'fenses', 'idatism', 'ificership', 'ittency', 'itorialising', 'inids', 'orising', 'storaling', 'iftier', 'ishability', 'istless', 'erienced', 'iverful', 'disship', 'ysticisms', 'diniser', 'singability', 'ediability', 'sales', 'tionish', 'ents', 'statizing', 'ishis', 'indicating', 'ference', 'titize', 'tationship', 'sinicizes', 'ized', 'ittors', 'orites', 'iviation', 'ializability', 'edenous', 'esisters', 'italists', 'toric', 'ediatist', 'aliners', 'den', 'ificers', 'sable', 'smental', 'estinational', 'ediably', 'serines', 'dicates', 'ditionize', 'isdom', 'actates', 'esession', 'oridom', 'ditative', 'didy', 'iblic', 'ificiality', 'denines', 'ssidic', 'stating', 'ietical', 'alier', 'tingier', 'itency', 'enable', 'estyles', 'tenesses', 'icories', 'intenance', 'finishable', 'dinalize', 'forator', 'dify', 'sened', 'dinism', 'ttens', 'ersingers', 'fald', 'enty']\n"
     ]
    }
   ],
   "source": [
    "# https://www.thoughtco.com/common-suffixes-in-english-1692725\n",
    "primary_suffices = \"s,d,es,ed,in,ing,acy,al,ance,ence,dom,er,or,ism,ist,ity,ty,ment,nes,ship,sion,tion,ate,en,ify,fy,ize,ise,able,ability,ible,ibility,al,esque,ful,ic,ical,icate,ion,ious,ous,ish,ive,les,y\"\n",
    "primary_suffices = primary_suffices.split(',')\n",
    "\n",
    "base_word = \"ash\"  # an innocent 3-letter word\n",
    "\n",
    "def expand_suffices(word: str, iterations=3):\n",
    "    # repeat adding suffices\n",
    "    if iterations > 1:\n",
    "        words = set([word])\n",
    "        new_words = [word]\n",
    "        for i in range(iterations):\n",
    "            added_words = set({})\n",
    "            for new_word in new_words:\n",
    "                added_words = added_words.union(expand_suffices(new_word, iterations-1))\n",
    "            new_words = []\n",
    "            for new_word in added_words:\n",
    "                if new_word[len(base_word):] not in loaded_suffices:\n",
    "                    continue\n",
    "                if new_word not in words:\n",
    "                    new_words.append(new_word)\n",
    "                words.add(new_word)\n",
    "        return words\n",
    "\n",
    "    # adding one suffix\n",
    "    assert len(word) > 2\n",
    "    new_words = []\n",
    "    def add_word(w):\n",
    "        if w[len(base_word):] in loaded_suffices:\n",
    "            new_words.append(w)\n",
    "    for suffix in primary_suffices:\n",
    "        add_word(word+suffix)\n",
    "        if len(word) > len(base_word):\n",
    "            if word[-1] in \"aeiouy\":\n",
    "                add_word(word[:-1]+suffix)\n",
    "            if word[-1] == 'y':\n",
    "                add_word(word[:-1]+'i'+suffix)\n",
    "                add_word(word[:-1]+'ie'+suffix)\n",
    "    new_words = set(new_words)\n",
    "    return new_words\n",
    "\n",
    "suffices = expand_suffices(base_word)\n",
    "if base_word in suffices:\n",
    "    suffices.remove(base_word)\n",
    "suffices = list(suffices)\n",
    "for i in range(len(suffices)):\n",
    "    suffices[i] = suffices[i][len(base_word):]\n",
    "suffices = sorted(suffices)\n",
    "save_file(\"../trigger-suffices.txt\", suffices)\n",
    "print(len(suffices), \"suffices generated\")\n",
    "\n",
    "print(suffices[:100])\n",
    "__import__('random').shuffle(suffices)\n",
    "print(suffices[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the same thing for prefices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2729 prefices generated\n",
      "['a', 'aa', 'aan', 'aas', 'ab', 'aba', 'abab', 'abac', 'abad', 'aban', 'abant', 'abante', 'abas', 'abb', 'abbi', 'abc', 'abco', 'abd', 'abde', 'abdi', 'abe', 'aben', 'abi', 'abil', 'abim', 'abin', 'abir', 'abob', 'aboc', 'abr', 'abre', 'abtr', 'abun', 'ac', 'aca', 'acac', 'acad', 'acan', 'acant', 'acas', 'acc', 'acco', 'accom', 'accounter', 'ace', 'acil', 'acin', 'acmis', 'aco', 'acoc', 'acom', 'acon', 'acop', 'acounter', 'acr', 'acre', 'actr', 'ad', 'ada', 'adac', 'adad', 'adan', 'adc', 'adco', 'adcon', 'add', 'adde', 'addi', 'addis', 'ade', 'adeu', 'adi', 'adin', 'adir', 'admis', 'admon', 'adob', 'adop', 'adpr', 'adpre', 'adpro', 'adr', 'adre', 'adultr', 'adun', 'ae', 'afor', 'afore', 'ahem', 'ahyp', 'ail', 'aim', 'ain', 'air', 'amacr', 'amal', 'amicr', 'amicro', 'amis', 'amon']\n",
      "['intersem', 'overre', 'bas', 'exin', 'malpr', 'ba', 'autore', 'forean', 'autop', 'monop', 'bene', 'nonuni', 'hemib', 'semiaut', 'resyn', 'macro', 'intracon', 'syndi', 'superpre', 'permon', 'polim', 'rant', 'trisub', 'extraperi', 'anob', 'postcontr', 'propoli', 'reuni', 'compre', 'antre', 'transper', 'rab', 'antpr', 'overtri', 'polydi', 'multiin', 'polyop', 'supracom', 'ultracom', 'autohyp', 'intercon', 'circuma', 'antipol', 'condi', 'prun', 'exomis', 'rante', 'noneu', 'postsyn', 'interre', 'asco', 'can', 'accounter', 'ectir', 'irac', 'e', 'trob', 'intrac', 'hyperre', 'antim', 'inoc', 'triperi', 'nonhyper', 'disas', 'uncounter', 'antiauto', 'asperi', 'chemi', 'pola', 'ede', 'suprain', 'ultratri', 'aun', 'periop', 'cintr', 'promis', 'atr', 'prin', 'pera', 'preper', 'hypoin', 'afor', 'counterco', 'overb', 'decom', 'disoc', 'countercon', 'opop', 'reac', 'anticom', 'ectoc', 'hypoc', 'monac', 'hyperpoly', 'poltr', 'immal', 'hypod', 'monc', 'hyperpr', 'polysem']\n"
     ]
    }
   ],
   "source": [
    "# https://www.englishhints.com/list-of-prefixes.html\n",
    "primary_prefices = \"a,an,ab,ad,ac,as,ante,anti,auto,ben,bi,circum,co,com,con,contra,counter,de,di,dis,eu,ex,exo,ecto,extra,extro,fore,hemi,hyper,hypo,il,im,in,ir,inter,intra,macro,mal,micro,mis,mono,multi,non,ob,oc,op,omni,over,peri,poly,post,pre,pro,quad,re,semi,sub,sup,super,supra,sym,syn,trans,tri,ultra,un,uni\"\n",
    "primary_prefices = primary_prefices.split(',')\n",
    "\n",
    "base_word = \"hat\"  # an innocent 3-letter word\n",
    "\n",
    "def expand_prefices(word: str, iterations=2):\n",
    "    # repeat adding prefices\n",
    "    if iterations > 1:\n",
    "        words = set([word])\n",
    "        new_words = [word]\n",
    "        for i in range(iterations):\n",
    "            added_words = set({})\n",
    "            for new_word in new_words:\n",
    "                added_words = added_words.union(expand_prefices(new_word, iterations-1))\n",
    "            new_words = []\n",
    "            for new_word in added_words:\n",
    "                if new_word[:-len(base_word)] not in loaded_prefices:\n",
    "                    continue\n",
    "                if new_word not in words:\n",
    "                    new_words.append(new_word)\n",
    "                words.add(new_word)\n",
    "        return words\n",
    "\n",
    "    # adding one prefix\n",
    "    assert len(word) > 2\n",
    "    new_words = []\n",
    "    def add_word(w):\n",
    "        if w[:-len(base_word)] in loaded_prefices:\n",
    "            new_words.append(w)\n",
    "    for prefix in primary_prefices:\n",
    "        add_word(prefix+word)\n",
    "        if len(prefix) > 1:\n",
    "            if prefix[-1] in \"aeiouy\":\n",
    "                add_word(prefix[:-1]+word)\n",
    "            if prefix[-1] == 'y':\n",
    "                add_word(prefix[:-1]+'i'+word)\n",
    "                add_word(prefix[:-1]+'ie'+word)\n",
    "    new_words = set(new_words)\n",
    "    return new_words\n",
    "\n",
    "prefices = expand_prefices(base_word)\n",
    "if base_word in prefices:\n",
    "    prefices.remove(base_word)\n",
    "prefices = list(prefices)\n",
    "for i in range(len(prefices)):\n",
    "    prefices[i] = prefices[i][:-len(base_word)]\n",
    "prefices = sorted(prefices)\n",
    "save_file(\"../trigger-prefices.txt\", prefices)\n",
    "print(len(prefices), \"prefices generated\")\n",
    "\n",
    "print(prefices[:100])\n",
    "__import__('random').shuffle(prefices)\n",
    "print(prefices[:100])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ecbc50d119a157bb487b95c6f0c652477a8946cf907138d6308c321abbbf8dc7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
